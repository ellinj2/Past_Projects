{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Text Generation with PyTorch Gated Recurrent Unit for the Shakespeare dataset\n",
    "\n",
    "#### To be run on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fVVb8nCnpvt1",
    "outputId": "e1b3eadd-8cd8-4dad-e7ca-fb898bb58ca4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time,math\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 (1) PyTorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fVVb8nCnpvt1",
    "outputId": "e1b3eadd-8cd8-4dad-e7ca-fb898bb58ca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Here\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyD7hLYmpvt1",
    "outputId": "491b76dc-8a2b-4863-97c0-6beb3704feda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y8GBDIl4pvt2"
   },
   "outputs": [],
   "source": [
    "class Data (Dataset):\n",
    "    def __init__(self,sequence_length):\n",
    "        self.seq_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "            'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "        text = open(path_to_file, 'rb').read().decode()\n",
    "        return text.replace(\"\\n\",\" \").split()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.seq_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.seq_length+1]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 50\n",
    "\n",
    "dataset = Data(sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 (2) Create the data loader with a mini-batch size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "batch = 256\n",
    "dataset_loader = DataLoader(dataset = dataset, batch_size=batch, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 (8) Create the GRU model class.\n",
    "\n",
    "* The size of the embedding vector should be 256\n",
    "\n",
    "* The number of layers in the GRU section should be 2\n",
    "\n",
    "* Enable a dropout of 20% in the GRU section\n",
    "\n",
    "* The output layer should be a linear layer\n",
    "\n",
    "* Hint: GRUs do not have a cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zdk5pZNNpvt2"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(GRU_Model, self).__init__()\n",
    "        self.gru_size = 256\n",
    "        self.embedding_size = 256\n",
    "        self.gru_layers = 2\n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_size,\n",
    "        )\n",
    "        self.GRU = nn.GRU(\n",
    "            input_size=self.gru_size,\n",
    "            hidden_size=self.gru_size,\n",
    "            num_layers=self.gru_layers,\n",
    "            dropout=self.drop_rate,\n",
    "        )\n",
    "        self.out = nn.Linear(self.gru_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.GRU(embed, prev_state)\n",
    "        logits = self.out(output)\n",
    "        \n",
    "        return logits, state\n",
    "    \n",
    "    def init_state(self, seq_length):\n",
    "        return torch.zeros(self.gru_layers, seq_length, self.gru_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4  (2) Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-x6gkb4v4ZjZ",
    "outputId": "403d97b1-7012-446e-85c7-587021b618a9"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "model = GRU_Model(dataset)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 (2) Use a CrossEntropy loss and the Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 (5) Create the training loop\n",
    "\n",
    "* After each epoch print the epoch number the perplexity and the loss\n",
    "\n",
    "* Use a mini-batch size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a60F4ODLpvt2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained\n",
      "Epoch 1 batch 1\n",
      "Epoch 1 batch 2\n",
      "Epoch 1 batch 3\n",
      "Epoch 1 batch 4\n",
      "Epoch 1 batch 5\n",
      "Epoch 1 batch 6\n",
      "Epoch 1 batch 7\n",
      "Epoch 1 batch 8\n",
      "Epoch 1 batch 9\n",
      "Epoch 1 batch 10\n",
      "Epoch 1 batch 11\n",
      "Epoch 1 batch 12\n",
      "Epoch 1 batch 13\n",
      "Epoch 1 batch 14\n",
      "Epoch 1 batch 15\n",
      "Epoch 1 batch 16\n",
      "Epoch 1 batch 17\n",
      "Epoch 1 batch 18\n",
      "Epoch 1 batch 19\n",
      "Epoch 1 batch 20\n",
      "Epoch 1 batch 21\n",
      "Epoch 1 batch 22\n",
      "Epoch 1 batch 23\n",
      "Epoch 1 batch 24\n",
      "Epoch 1 batch 25\n",
      "Epoch 1 batch 26\n",
      "Epoch 1 batch 27\n",
      "Epoch 1 batch 28\n",
      "Epoch 1 batch 29\n",
      "Epoch 1 batch 30\n",
      "Epoch 1 batch 31\n",
      "Epoch 1 batch 32\n",
      "Epoch 1 batch 33\n",
      "Epoch 1 batch 34\n",
      "Epoch 1 batch 35\n",
      "Epoch 1 batch 36\n",
      "Epoch 1 batch 37\n",
      "Epoch 1 batch 38\n",
      "Epoch 1 batch 39\n",
      "Epoch 1 batch 40\n",
      "Epoch 1 batch 41\n",
      "Epoch 1 batch 42\n",
      "Epoch 1 batch 43\n",
      "Epoch 1 batch 44\n",
      "Epoch 1 batch 45\n",
      "Epoch 1 batch 46\n",
      "Epoch 1 batch 47\n",
      "Epoch 1 batch 48\n",
      "Epoch 1 batch 49\n",
      "Epoch 1 batch 50\n",
      "Epoch 1 batch 51\n",
      "Epoch 1 batch 52\n",
      "Epoch 1 batch 53\n",
      "Epoch 1 batch 54\n",
      "Epoch 1 batch 55\n",
      "Epoch 1 batch 56\n",
      "Epoch 1 batch 57\n",
      "Epoch 1 batch 58\n",
      "Epoch 1 batch 59\n",
      "Epoch 1 batch 60\n",
      "Epoch 1 batch 61\n",
      "Epoch 1 batch 62\n",
      "Epoch 1 batch 63\n",
      "Epoch 1 batch 64\n",
      "Epoch 1 batch 65\n",
      "Epoch 1 batch 66\n",
      "Epoch 1 batch 67\n",
      "Epoch 1 batch 68\n",
      "Epoch 1 batch 69\n",
      "Epoch 1 batch 70\n",
      "Epoch 1 batch 71\n",
      "Epoch 1 batch 72\n",
      "Epoch 1 batch 73\n",
      "Epoch 1 batch 74\n",
      "Epoch 1 batch 75\n",
      "Epoch 1 batch 76\n",
      "Epoch 1 batch 77\n",
      "Epoch 1 batch 78\n",
      "Epoch 1 batch 79\n",
      "Epoch 1 batch 80\n",
      "Epoch 1 batch 81\n",
      "Epoch 1 batch 82\n",
      "Epoch 1 batch 83\n",
      "Epoch 1 batch 84\n",
      "Epoch 1 batch 85\n",
      "Epoch 1 batch 86\n",
      "Epoch 1 batch 87\n",
      "Epoch 1 batch 88\n",
      "Epoch 1 batch 89\n",
      "Epoch 1 batch 90\n",
      "Epoch 1 batch 91\n",
      "Epoch 1 batch 92\n",
      "Epoch 1 batch 93\n",
      "Epoch 1 batch 94\n",
      "Epoch 1 batch 95\n",
      "Epoch 1 batch 96\n",
      "Epoch 1 batch 97\n",
      "Epoch 1 batch 98\n",
      "Epoch 1 batch 99\n",
      "Epoch 1 batch 100\n",
      "Epoch 1 batch 101\n",
      "Epoch 1 batch 102\n",
      "Epoch 1 batch 103\n",
      "Epoch 1 batch 104\n",
      "Epoch 1 batch 105\n",
      "Epoch 1 batch 106\n",
      "Epoch 1 batch 107\n",
      "Epoch 1 batch 108\n",
      "Epoch 1 batch 109\n",
      "Epoch 1 batch 110\n",
      "Epoch 1 batch 111\n",
      "Epoch 1 batch 112\n",
      "Epoch 1 batch 113\n",
      "Epoch 1 batch 114\n",
      "Epoch 1 batch 115\n",
      "Epoch 1 batch 116\n",
      "Epoch 1 batch 117\n",
      "Epoch 1 batch 118\n",
      "Epoch 1 batch 119\n",
      "Epoch 1 batch 120\n",
      "Epoch 1 batch 121\n",
      "Epoch 1 batch 122\n",
      "Epoch 1 batch 123\n",
      "Epoch 1 batch 124\n",
      "Epoch 1 batch 125\n",
      "Epoch 1 batch 126\n",
      "Epoch 1 batch 127\n",
      "Epoch 1 batch 128\n",
      "Epoch 1 batch 129\n",
      "Epoch 1 batch 130\n",
      "Epoch 1 batch 131\n",
      "Epoch 1 batch 132\n",
      "Epoch 1 batch 133\n",
      "Epoch 1 batch 134\n",
      "Epoch 1 batch 135\n",
      "Epoch 1 batch 136\n",
      "Epoch 1 batch 137\n",
      "Epoch 1 batch 138\n",
      "Epoch 1 batch 139\n",
      "Epoch 1 batch 140\n",
      "Epoch 1 batch 141\n",
      "Epoch 1 batch 142\n",
      "Epoch 1 batch 143\n",
      "Epoch 1 batch 144\n",
      "Epoch 1 batch 145\n"
     ]
    }
   ],
   "source": [
    "# Your Code Here\n",
    "max_epochs = 20\n",
    "model.train()\n",
    "\n",
    "print(\"Model trained\")\n",
    "\n",
    "start=time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    state_h = model.init_state(sequence_length)\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for batch, (x,y) in enumerate(dataset_loader):\n",
    "        print(f\"Epoch {epoch+1} batch {batch+1}\")\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred, state_h = model(x, (state_h.to(device)))\n",
    "        loss = criterion(y_pred.transpose(1, 2), y)\n",
    "        \n",
    "        state_h = state_h.detach()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * y.numel()\n",
    "        n += y.numel()\n",
    "    pp = np.round(math.exp(loss_sum / n))\n",
    "    print(f\"epoch {epoch+1} time {np.round(time.time()-start,2)} sec perplexity {pp} loss {loss.item()}\")\n",
    "    start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 (5) Predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lxXaZ50pvt2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(dataset, model, text, next_words=100):\n",
    "    # Your Code Here\n",
    "    words = text.split(' ')\n",
    "    state_h = model.init_state(len(words))\n",
    "    \n",
    "    for i in range(next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, state_h = model(x, (state_h.to(device)))\n",
    "        \n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).cpu().detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g83T64uepvt2"
   },
   "outputs": [],
   "source": [
    "words = predict(dataset, model, text='Romeo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "bgHqRTKwpvt2",
    "outputId": "5a4b38e1-3832-4b51-9871-4177e96f42f5"
   },
   "outputs": [],
   "source": [
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_HW4_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
