{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Text Generation with PyTorch Gated Recurrent Unit for the Shakespeare dataset\n",
    "\n",
    "#### To be run on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fVVb8nCnpvt1",
    "outputId": "e1b3eadd-8cd8-4dad-e7ca-fb898bb58ca4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import time,math\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 (1) PyTorch imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fVVb8nCnpvt1",
    "outputId": "e1b3eadd-8cd8-4dad-e7ca-fb898bb58ca4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your Code Here\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyD7hLYmpvt1",
    "outputId": "491b76dc-8a2b-4863-97c0-6beb3704feda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y8GBDIl4pvt2"
   },
   "outputs": [],
   "source": [
    "class Data (Dataset):\n",
    "    def __init__(self,sequence_length):\n",
    "        self.seq_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "            'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "        text = open(path_to_file, 'rb').read().decode()\n",
    "        return text.replace(\"\\n\",\" \").split()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.seq_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.seq_length+1]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 50\n",
    "\n",
    "dataset = Data(sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 (2) Create the data loader with a mini-batch size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "batch = 256\n",
    "dataset_loader = DataLoader(dataset = dataset, batch_size=batch, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 (8) Create the GRU model class.\n",
    "\n",
    "* The size of the embedding vector should be 256\n",
    "\n",
    "* The number of layers in the GRU section should be 2\n",
    "\n",
    "* Enable a dropout of 20% in the GRU section\n",
    "\n",
    "* The output layer should be a linear layer\n",
    "\n",
    "* Hint: GRUs do not have a cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zdk5pZNNpvt2"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "class GRU_Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(GRU_Model, self).__init__()\n",
    "        self.gru_size = 256\n",
    "        self.embedding_size = 256\n",
    "        self.gru_layers = 2\n",
    "        self.drop_rate = 0.2\n",
    "        \n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_size,\n",
    "        )\n",
    "        self.GRU = nn.GRU(\n",
    "            input_size=self.gru_size,\n",
    "            hidden_size=self.gru_size,\n",
    "            num_layers=self.gru_layers,\n",
    "            dropout=self.drop_rate,\n",
    "        )\n",
    "        self.out = nn.Linear(self.gru_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.GRU(embed, prev_state)\n",
    "        logits = self.out(output)\n",
    "        \n",
    "        return logits, state\n",
    "    \n",
    "    def init_state(self, seq_length):\n",
    "        return torch.zeros(self.gru_layers, seq_length, self.gru_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4  (2) Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-x6gkb4v4ZjZ",
    "outputId": "403d97b1-7012-446e-85c7-587021b618a9"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "model = GRU_Model(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 (2) Use a CrossEntropy loss and the Adam optimizer with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 (5) Create the training loop\n",
    "\n",
    "* After each epoch print the epoch number the perplexity and the loss\n",
    "\n",
    "* Use a mini-batch size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a60F4ODLpvt2"
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "max_epochs = 20\n",
    "model.train()\n",
    "\n",
    "start=time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    state_h = model.init_state(sequence_length)\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for batch, (x,y) in enumerate(dataset_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred, state_h = model(x, (state_h.to(device)))\n",
    "        loss = criterion(y_pred.transpose(1, 2), y)\n",
    "        \n",
    "        state_h = state_h.detach()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * y.numel()\n",
    "        n += y.numel()\n",
    "    pp = np.round(math.exp(loss_sum / n))\n",
    "    print(f\"epoch {epoch+1} time {np.round(time.time()-start,2)} sec perplexity {pp} loss {loss.item()}\")\n",
    "    start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 (5) Predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lxXaZ50pvt2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(dataset, model, text, next_words=100):\n",
    "    # Your Code Here\n",
    "    words = text.split(' ')\n",
    "    state_h = model.init_state(len(words))\n",
    "    \n",
    "    for i in range(next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]]).to(device)\n",
    "        y_pred, state_h = model(x, (state_h.to(device)))\n",
    "        \n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).cpu().detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "    \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g83T64uepvt2"
   },
   "outputs": [],
   "source": [
    "words = predict(dataset, model, text='Romeo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "bgHqRTKwpvt2",
    "outputId": "5a4b38e1-3832-4b51-9871-4177e96f42f5"
   },
   "outputs": [],
   "source": [
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_HW4_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
